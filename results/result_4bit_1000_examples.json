{
    "professional_psychology": {
        "accuracy": 0.6216216216216216,
        "number_examples": 37,
        "execution_time": 1338.125,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "philosophy": {
        "accuracy": 0.6956521739130435,
        "number_examples": 23,
        "execution_time": 605.149,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "high_school_chemistry": {
        "accuracy": 0.75,
        "number_examples": 16,
        "execution_time": 1060.527,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "high_school_government_and_politics": {
        "accuracy": 0.5384615384615384,
        "number_examples": 13,
        "execution_time": 357.381,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "abstract_algebra": {
        "accuracy": 0.6363636363636364,
        "number_examples": 11,
        "execution_time": 577.67,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "management": {
        "accuracy": 0.42857142857142855,
        "number_examples": 7,
        "execution_time": 185.027,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "moral_disputes": {
        "accuracy": 0.55,
        "number_examples": 20,
        "execution_time": 563.733,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "jurisprudence": {
        "accuracy": 0.8,
        "number_examples": 10,
        "execution_time": 390.337,
        "used_VRAM": 2222,
        "total_VRAM": 8188
    },
    "professional_medicine": {
        "accuracy": 0.5882352941176471,
        "number_examples": 17,
        "execution_time": 753.292,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_physics": {
        "accuracy": 0.6153846153846154,
        "number_examples": 13,
        "execution_time": 720.644,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "business_ethics": {
        "accuracy": 0.2,
        "number_examples": 5,
        "execution_time": 300.168,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "world_religions": {
        "accuracy": 0.9090909090909091,
        "number_examples": 11,
        "execution_time": 311.767,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "medical_genetics": {
        "accuracy": 0.75,
        "number_examples": 8,
        "execution_time": 263.792,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "college_computer_science": {
        "accuracy": 0.625,
        "number_examples": 8,
        "execution_time": 202.064,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "international_law": {
        "accuracy": 0.2222222222222222,
        "number_examples": 9,
        "execution_time": 451.33,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "college_mathematics": {
        "accuracy": 0.8,
        "number_examples": 5,
        "execution_time": 296.966,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "college_chemistry": {
        "accuracy": 0.375,
        "number_examples": 8,
        "execution_time": 527.719,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "marketing": {
        "accuracy": 0.8235294117647058,
        "number_examples": 17,
        "execution_time": 341.345,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "clinical_knowledge": {
        "accuracy": 0.6666666666666666,
        "number_examples": 15,
        "execution_time": 306.608,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "conceptual_physics": {
        "accuracy": 0.8260869565217391,
        "number_examples": 23,
        "execution_time": 725.323,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "professional_law": {
        "accuracy": 0.3761467889908257,
        "number_examples": 109,
        "execution_time": 6910.658,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_mathematics": {
        "accuracy": 0.75,
        "number_examples": 20,
        "execution_time": 1444.183,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "electrical_engineering": {
        "accuracy": 0.5,
        "number_examples": 10,
        "execution_time": 494.337,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "econometrics": {
        "accuracy": 0.3333333333333333,
        "number_examples": 9,
        "execution_time": 431.022,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_psychology": {
        "accuracy": 0.7297297297297297,
        "number_examples": 37,
        "execution_time": 836.691,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_geography": {
        "accuracy": 0.6666666666666666,
        "number_examples": 12,
        "execution_time": 424.359,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "us_foreign_policy": {
        "accuracy": 0.5,
        "number_examples": 4,
        "execution_time": 101.031,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "college_biology": {
        "accuracy": 0.375,
        "number_examples": 8,
        "execution_time": 549.933,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "miscellaneous": {
        "accuracy": 0.5964912280701754,
        "number_examples": 57,
        "execution_time": 1236.79,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_macroeconomics": {
        "accuracy": 0.52,
        "number_examples": 25,
        "execution_time": 1120.602,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "college_medicine": {
        "accuracy": 0.6666666666666666,
        "number_examples": 12,
        "execution_time": 568.121,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_world_history": {
        "accuracy": 0.8260869565217391,
        "number_examples": 23,
        "execution_time": 637.904,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "public_relations": {
        "accuracy": 0.375,
        "number_examples": 8,
        "execution_time": 178.056,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "moral_scenarios": {
        "accuracy": 0.5147058823529411,
        "number_examples": 68,
        "execution_time": 3354.058,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "sociology": {
        "accuracy": 0.6363636363636364,
        "number_examples": 11,
        "execution_time": 349.278,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "human_sexuality": {
        "accuracy": 0.625,
        "number_examples": 8,
        "execution_time": 328.932,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "computer_security": {
        "accuracy": 0.6,
        "number_examples": 5,
        "execution_time": 97.067,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "machine_learning": {
        "accuracy": 0.5,
        "number_examples": 6,
        "execution_time": 170.596,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "formal_logic": {
        "accuracy": 0.5,
        "number_examples": 10,
        "execution_time": 764.534,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_biology": {
        "accuracy": 0.782608695652174,
        "number_examples": 23,
        "execution_time": 565.34,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "human_aging": {
        "accuracy": 0.5882352941176471,
        "number_examples": 17,
        "execution_time": 317.49,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "elementary_mathematics": {
        "accuracy": 0.875,
        "number_examples": 24,
        "execution_time": 909.095,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_computer_science": {
        "accuracy": 0.8333333333333334,
        "number_examples": 6,
        "execution_time": 214.672,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "college_physics": {
        "accuracy": 0.75,
        "number_examples": 12,
        "execution_time": 651.538,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "security_studies": {
        "accuracy": 0.6111111111111112,
        "number_examples": 18,
        "execution_time": 485.093,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "nutrition": {
        "accuracy": 0.65,
        "number_examples": 20,
        "execution_time": 765.63,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "astronomy": {
        "accuracy": 0.6,
        "number_examples": 5,
        "execution_time": 102.629,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "anatomy": {
        "accuracy": 0.5555555555555556,
        "number_examples": 9,
        "execution_time": 448.286,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_us_history": {
        "accuracy": 0.6875,
        "number_examples": 16,
        "execution_time": 600.212,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "virology": {
        "accuracy": 0.42857142857142855,
        "number_examples": 14,
        "execution_time": 426.002,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "professional_accounting": {
        "accuracy": 0.56,
        "number_examples": 25,
        "execution_time": 1410.401,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "global_facts": {
        "accuracy": 0.2,
        "number_examples": 5,
        "execution_time": 124.707,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_statistics": {
        "accuracy": 0.375,
        "number_examples": 16,
        "execution_time": 1184.738,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "prehistory": {
        "accuracy": 0.5588235294117647,
        "number_examples": 34,
        "execution_time": 1171.272,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_microeconomics": {
        "accuracy": 0.7333333333333333,
        "number_examples": 15,
        "execution_time": 628.221,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "logical_fallacies": {
        "accuracy": 0.8,
        "number_examples": 10,
        "execution_time": 247.417,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    },
    "high_school_european_history": {
        "accuracy": 0.6153846153846154,
        "number_examples": 13,
        "execution_time": 550.238,
        "used_VRAM": 2224,
        "total_VRAM": 8188
    }
}